{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 5: Logistic Regression\n",
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "$\\renewcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\renewcommand{\\wb}{\\mathbf{w}}$\n",
    "$\\renewcommand{\\Xb}{\\mathbf{X}}$\n",
    "$\\renewcommand{\\yb}{\\mathbf{y}}$\n",
    "$\\renewcommand{\\Yb}{\\mathbf{Y}}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "# project files\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import hw_05.helpers as helpers\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "This week's exercise is about linear classification, in particular, logistic regression. You will see both the binary and the multi-class variant of the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Binary Class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the [_Iris Flower Dataset_](https://en.wikipedia.org/wiki/Iris_flower_data_set). Since our topic is linear classification, we will only use 2 out of 4 features of this dataset. For the first part we will use 2 out of 3 classes (named as *setosa* and *versicolor*). Later on, we will also use the third class *virginica*.\n",
    "Therefore, for this part our dataset with two classes is as follows:\n",
    "\n",
    "  - data: $\\Xb \\in \\real^{N \\times 3}$, $\\forall \\xb_i \\in \\Xb: \\xb_i \\in \\real^{3}$ (2 features and the bias)\n",
    "  - labels: $\\yb \\in \\real^{N}$, $\\forall y_i \\in \\yb: y_i \\in \\{0, 1\\}$ \n",
    "\n",
    "Note that $\\Xb$ is a matrix of shape $(N \\times D)$. However, a single data sample $\\xb_i$ is a column vector of shape $(D \\times 1)$. \n",
    "When you want to perform a scalar product of one data sample with the weights vector $\\wb$ (also a column vector of shape $(D \\times 1)$) you will see: $\\xb_i^\\top\\cdot\\wb$. When you want to perform matrix-vector multiplication of the entire data matrix with the weights vector, you will see: $\\Xb\\cdot\\wb$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data and split them into trian and test subsets.\n",
    "data, labels = helpers.load_ds_iris(sep_l=True, sep_w=True, pet_l=False, pet_w=False,\n",
    "                              setosa=True, versicolor=True, virginica=False, addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data, labels)\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "\n",
    "data_train = data[rinds[:int(num_samples * fraction_train)]] \n",
    "labels_train = labels[rinds[:int(num_samples * fraction_train)]]  \n",
    "\n",
    "data_test = data[rinds[int(num_samples * fraction_train):]] \n",
    "labels_test = labels[rinds[int(num_samples * fraction_train):]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A short introduction\n",
    "\n",
    "In logistic regression, the probability of a datapoint belonging to a class is found as:\n",
    "$$P(y_i=1|\\xb_i, \\wb) = \\frac{1}{1+e^{-\\xb_i^{\\top}\\cdot \\wb}} $$\n",
    "\n",
    "This is called the sigmoid function! The sigmoid function is defined as:\n",
    "$$\\sigma(t)= \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "So in our case, our model is defined as:\n",
    "$$\\hat{y}(\\xb_i)=\\sigma(\\xb_i^{\\top}\\cdot \\wb)= \\frac{1}{1+e^{-\\xb_i^{\\top}\\cdot \\wb}}$$\n",
    "\n",
    "\n",
    "\n",
    "Let's try to code this function. You can use the numpy function `np.exp(x)` to take the exponential of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\" Sigmoid function\n",
    "    \n",
    "    Args:\n",
    "        t (np.array): Input data of shape (N, )\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Probabilites of shape (N, ), where each value is in [0, 1].\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the cross entropy loss is defined as:\n",
    "$$\n",
    "R(\\wb) = -\\sum_i (y_i \\log(\\hat{y}(\\xb_i)) + (1-y_i)\\log(1-\\hat{y}(\\xb_i))) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_logistic(data, labels, w): \n",
    "    \"\"\" Logistic regression loss function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        int: Loss of logistic regression.\n",
    "    \"\"\"    \n",
    "    return -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the gradient of the loss function in order to move our weights towards the optimal weights. The gradient of the loss function is:\n",
    "$$\\nabla R(\\wb)= \\sum_i (\\hat{y}(\\xb_i) - y_i)\\xb_i $$\n",
    "Let us put this into nice matrix format:\n",
    "$$\\nabla R(\\wb)= \\Xb^\\top(\\hat{y}(\\Xb) - \\yb) = \\Xb^\\top(\\sigma(\\Xb\\cdot \\wb) - \\yb),\n",
    "$$\n",
    "\n",
    "where $\\hat{y}(\\Xb) = \\sigma(\\Xb\\cdot \\wb)$ and $\\sigma(\\Xb\\cdot \\wb)$ computes the sigmoid for each data sample separately, and returns a vector of shape $(N \\times 1)$.\n",
    "\n",
    "Fill in the function for finding the gradient `gradient_logistic()`. You can use the numpy function `np.dot()` or an operator `@` for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic(data, labels, w):\n",
    "    \"\"\" Logistic regression gradient function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        np. array: Gradient array of shape (D, )\n",
    "    \"\"\"\n",
    "    return data.T.dot(sigmoid(data.dot(w))-labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classifying using a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us write a function to perform classification using logistic regression, `logistic_regression_classify()`. This function uses the weights we find during training in order to predict the labels for the data.\n",
    "\n",
    "**Hints:**\n",
    "* We classify our data according to $P(y_i=1|\\xb_i, \\wb)$. If the value of $P(y_i=1|\\xb_i, \\wb)$ is less than 0.5 then the data point is classified as label 0. If it is more than or equal to 0.5 then we classify the data point as label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classify(data, w):\n",
    "    \"\"\" Classification function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        np.array: Label assignments of data of shape (N, )\n",
    "    \"\"\"\n",
    "    #### write your code here: find predictions and threshold.\n",
    "    predictions = sigmoid(data@w)\n",
    "    predictions[predictions<0.5]=0\n",
    "    predictions[predictions>=0.5]=1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also use our `accuracy()` function from last week's exercise. Recall that it implements\n",
    "$$ f_{\\text{acc}} = \\frac{\\text{# correct predictions}}{\\text{# all predictions}}$$\n",
    "We have provided it for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels_gt, labels_pred):\n",
    "    \"\"\" Computes accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels_gt (np.array): GT labels of shape (N, ).\n",
    "        labels_pred (np.array): Predicted labels of shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    np.sum(np.abs(labels_gt - labels_pred)==0)\n",
    "    \n",
    "    return np.sum(labels_gt == labels_pred) / labels_gt.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find good weights and have a high accuracy we need to train our model. Fill in missing parts of the function `logistic_regression_train()`.\n",
    "\n",
    "The function first initializes the weights according to a gaussian distribution. In each iteration, you should find the gradient using `gradient_logistic` and take a gradient step using these weights. Given that $\\eta$ is the learning rate, recall that a gradient step is: $$ \\wb^{[t + 1]}  = \\wb^{[t]} - \\eta \\nabla R(\\wb^{[t]}) $$\n",
    "\n",
    "The `loss`, `plot` and `print_every` parameters affect the way the loss is printed and the predictions are displayed. You do not need to modify these parts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(data, labels, max_iters=10, lr=0.001, \n",
    "                              print_period=1000, plot_period=1000):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        max_iters (integer): Maximum number of iterations. Default:10\n",
    "        lr (integer): The learning rate of  the gradient step. Default:0.001\n",
    "        print_period (int): Num. iterations to print current loss. \n",
    "            If 0, never printed.\n",
    "        plot_period (int): Num. iterations to plot current predictions.\n",
    "            If 0, never plotted.\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize weights randomly according to gaussian distribution\n",
    "    weights = np.random.normal(0., 0.1, [data.shape[1],])\n",
    "    for it in range(max_iters):\n",
    "        ########## write your code here: find gradient and do a gradient step\n",
    "        gradient = gradient_logistic(data, labels, weights)\n",
    "        weights = weights - lr*gradient\n",
    "        ##################################\n",
    "        predictions = logistic_regression_classify(data, weights)\n",
    "        if print_period and it % print_period == 0:\n",
    "            print('loss at iteration', it, \":\", loss_logistic(data, labels, weights))\n",
    "        if plot_period and it % plot_period == 0:\n",
    "            fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions)\n",
    "            plt.title(\"iteration \"+ str(it))\n",
    "        if accuracy(labels, predictions) == 1:\n",
    "            break\n",
    "    fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions)\n",
    "    plt.title(\"iteration \"+ str(it))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see your training in action. What do you observe? Try playing with the learning rate and number of max iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = logistic_regression_train(data_train, labels_train, max_iters=100000, lr=1e-2, print_period=1000, plot_period=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_regression_classify(data_test, weights)\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=labels_test, labels_pred=predictions)\n",
    "plt.title(\"test result\")\n",
    "print(\"Accuracy is\", accuracy(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have classified two classes, we can move on to multi-class logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Multi-Class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the synthetic data by running the code segment below. We will use this dataset for now as it is easy to work with.\n",
    "Our data is:\n",
    "\n",
    " - data: $\\Xb \\in \\real^{N \\times 3}$, $\\forall \\xb_i \\in \\Xb: \\xb_i \\in \\real^{3}$ (2 features + the bias)\n",
    " - labels: $\\Yb \\in \\real^{N \\times C}$, $\\forall \\yb_i \\in \\Yb: \\yb_i$  is a one-hot encoding of the label of a data sample, e.g. $\\yb_i = [0, 0, 1]$ if $\\xb_i$ is class $2$. $C$ is number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers.generate_dataset_synth()\n",
    "data_multi, labels_multi = helpers.load_dataset_synth(addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data_multi, helpers.onehot_to_label(labels_multi), fig=None, fig_size=None, color_map=None,\n",
    "                         legend=True, legend_map=None, grid=False, show=False)\n",
    "\n",
    "num_samples = data_multi.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "data_train = data_multi[rinds[:int(num_samples * fraction_train)]] \n",
    "labels_train = labels_multi[rinds[:int(num_samples * fraction_train)]]  \n",
    "\n",
    "data_test = data_multi[rinds[int(num_samples * fraction_train):]] \n",
    "labels_test= labels_multi[rinds[int(num_samples * fraction_train):]]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 A short introduction \n",
    "\n",
    "Multi class logistic regression is an extention to binary logistic regression.\n",
    "\n",
    "Let us consider logistic regression for C classes. We keep our weights in a weight matrix $\\mathbf{W}$, where every column is $\\wb_{(k)}$ for class $k$. Therefore, for every class $k$, we learn a separate $\\wb_{(k)}$ during training. The weights matrix will be of size $(3 \\times C)$.\n",
    "\n",
    "The generalized probabilities for logistic regression is\n",
    "$$\\hat{y}^{(k)}(\\xb_i) = P(y_i=k|\\xb_i, \\mathbf{W}) = \\frac{e^{\\xb_i^\\top\\cdot \\wb_{(k)}}}{\\sum_j^C e^{\\xb_i^\\top\\cdot \\wb_{(j)}}}$$ \n",
    "which is called the softmax function! Let us denote this function by $f_\\text{softmax}$.\n",
    "\n",
    "Fill in the implementation of this function below. It is used to assign the probabilities of a datapoint belonging to each class. For example, for a single datapoint, for 3 classes you might have the following probability assignments {0.2, 0.7, 0.1}. The probabilities all sum up to 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_softmax(data, w):\n",
    "    \"\"\" Softmax function\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Input data of shape (N, D)\n",
    "        w (np.array): Weights of shape (D, C) where C is # of classes\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Probabilites of shape (N, C), where each value is in \n",
    "            range [0, 1] and each row sums to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    #data.shape[0] is the number of datapoints, and w.shape[1] is the number of classes.\n",
    "    res = np.zeros([data.shape[0], w.shape[1]]) \n",
    "    #The normalization term only has to be calculated once for all classes\n",
    "    norm = np.sum(np.exp(data@w), axis=1)\n",
    "    #We iterate for each class\n",
    "    for k in range(w.shape[1]):\n",
    "        exp_top = np.exp(data@w[:, k])\n",
    "        res[:, k] = exp_top/norm\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these, we find the loss function which we are trying to minimize is\n",
    "\n",
    "$$R(\\mathbf{W}) = -\\sum_{i}^N\\sum_{k}^Cy_i^{(k)}\\log(\\hat{y}^{(k)}(\\xb_i)) \\\\\n",
    "=-\\sum_{i}^N\\sum_{k}^Cy_i^{(k)}\\log(f_\\text{softmax}(\\xb_{i}^\\top \\cdot \\wb_{(k)}))$$ \n",
    "\n",
    "Fill in the loss function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_logistic_multi(data, labels, w):\n",
    "    \"\"\" Loss function for multi class logistic regression\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Input data of shape (N, D)\n",
    "        labels (np.array): Labels of shape  (N, C)\n",
    "        w (np.array): Weights of shape (D, C)\n",
    "        \n",
    "    Returns:\n",
    "        float: Loss value \n",
    "    \"\"\"\n",
    "    res = f_softmax(data,w)\n",
    "    loss = - np.sum(np.log(res[labels.astype(np.bool)]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the gradient, we find the gradient of $R(\\mathbf{W})$ with respect to the weights $\\wb$. We have\n",
    "$$\\nabla R(\\mathbf{W})=\\sum_i^N\\xb_{i}(\\hat{\\yb}(\\xb_i)-y_i)^\\top$$\n",
    "Let's put this into matrix format as well:\n",
    "$$\\nabla R(\\mathbf{W})= \\Xb^T(\\hat{\\yb}(\\Xb) - \\Yb)$$\n",
    "\n",
    "A note on the notation:\n",
    "Here, $\\hat{\\yb}(\\xb_i)$ returns the softmax result of shape $(C \\times 1)$ for sample $\\xb_i$ for all classes. \n",
    "\n",
    "$\\hat{\\yb}(\\Xb)$ should return a matrix of shape $(N\\times C)$, which consists of the softmax predictions for every sample for all classes. (Each row of $\\hat{\\yb}(\\Xb)$ is $\\hat{\\yb}(\\xb_i)$.)\n",
    "\n",
    "\n",
    "Now, you will fill in the gradient function, `gradient_logistic_multi()` given below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic_multi(data, labels, w):\n",
    "    \"\"\" Gradient function for multi class logistic regression\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Input data of shape (N, D)\n",
    "        labels (np.array): Labels of shape  (N, )\n",
    "        w (np.array): Weights of shape (D, C)\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Gradients of shape (D, C)\n",
    "    \"\"\"\n",
    "    grad_w = data.T@(f_softmax(data,w)-labels)\n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Classification and training for multiple classes\n",
    "\n",
    "Write the functions for classification and training.\n",
    "\n",
    "Hints:\n",
    "* For the classification function, you will be using $f_\\text{softmax}$ to assign the probabilities of a datapoint belonging to each class. The softmax function returns an array of size $(N \\times C)$.\n",
    "* You will have to convert one-hot representation to labels (`np.argmax` is your friend). \n",
    "\n",
    "* Training will be the same as the binary case. First, we will find the gradient. Then we will update the weights using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classify_multi(data, w):\n",
    "    \"\"\" Classification function for multi class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, C)\n",
    "    Returns:\n",
    "        np. array: Label assignments of data of shape (N, ).\n",
    "    \"\"\"\n",
    "    #### write your code here: find predictions, argmax to find the correct label\n",
    "    predictions = np.argmax(f_softmax(data, w), axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train_multi(data, labels, k=3, max_iters=10, lr=0.001, \n",
    "                                    print_period=5, plot_period=5):\n",
    "    \"\"\" Classification function for multi class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, )\n",
    "        k (integer): Number of classes. Default=3\n",
    "        max_iters (integer): Maximum number of iterations. Default:10\n",
    "        lr (integer): The learning rate of  the gradient step. Default:0.001\n",
    "        print_period (int): Num. iterations to print current loss. \n",
    "            If 0, never printed.\n",
    "        plot_period (int): Num. iterations to plot current predictions.\n",
    "            If 0, never plotted.\n",
    "\n",
    "    Returns:\n",
    "        np. array: Label assignments of data of shape (N, ).\n",
    "    \"\"\"\n",
    "    weights = np.random.normal(0, 0.1, [data.shape[1], k])\n",
    "    for it in range(max_iters):\n",
    "        gradient = gradient_logistic_multi(data, labels, weights)\n",
    "        weights = weights - lr*gradient\n",
    "        ##################################\n",
    "        predictions = logistic_regression_classify_multi(data, weights)\n",
    "        if print_period and it % print_period == 0:\n",
    "            print('loss at iteration', it, \":\", loss_logistic_multi(data, labels, weights))\n",
    "        if plot_period and it % plot_period == 0:\n",
    "            fig = helpers.visualize_predictions(data=data, labels_gt=helpers.onehot_to_label(labels), labels_pred=predictions)\n",
    "            plt.title(\"iteration \"+ str(it))\n",
    "        if accuracy(helpers.onehot_to_label(labels), predictions) == 1:\n",
    "            break\n",
    "    fig = helpers.visualize_predictions(data=data, labels_gt=helpers.onehot_to_label(labels), labels_pred=predictions)\n",
    "    plt.title(\"iteration \"+ str(it))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_multi = logistic_regression_train_multi(data_train, labels_train, max_iters=20, lr=1e-3, print_period=5, plot_period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_multi = logistic_regression_classify_multi(data_test, weights_multi)\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=helpers.onehot_to_label(labels_test), labels_pred=predictions_multi)\n",
    "plt.title(\"test result\")\n",
    "print(\"Accuracy is\", accuracy(helpers.onehot_to_label(labels_test), predictions_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A side note:** Notice that using this simple formulation, we have trained C classifiers for C classes.\n",
    "Our probability assignments are according to the softmax function.\n",
    "$$P(\\yb_i=k|\\xb_i, \\mathbf{W}) = \\frac{e^{\\xb_i^\\top\\cdot \\wb_{(k)}}}{\\sum_j^C e^{\\xb_i^\\top\\cdot \\wb_{(j)}}}$$\n",
    "And $$\\sum_{k}^{C}P(\\yb_i=k|\\xb_i, \\mathbf{W})=1$$\n",
    "However, in the binary case we were training 1 classifier for 2 classes. The probabilities are assigned according to the sigmoid function.\n",
    "$$P(\\yb_i=1|\\xb_i, \\wb) = \\frac{1}{1+e^{-(\\xb_i^\\top\\cdot \\wb)}} \\\\\n",
    "P(\\yb_i=0|\\xb_i, \\wb) = 1-P(\\yb_i=1|\\xb_i, \\wb) = \\frac{1}{1+e^{(\\xb_i^\\top\\cdot \\wb)}}$$\n",
    "\n",
    "Similar to the binary case, we can train C-1 classifiers for C classes. We modify the probability assignment function to be, for classes $k={1, ... ,C-1}$.\n",
    "$$P(\\yb_i=k|\\xb_i, \\mathbf{W}) = \\frac{e^{\\xb_i^\\top\\cdot \\wb_{(k)}}}{1+\\sum_j^{C-1} e^{\\xb_i^\\top\\cdot \\wb_{(j)}}}$$\n",
    "\n",
    "**Q: What is $P(\\yb_n=C|\\xb_i, \\mathbf{W})$?**\n",
    "$$P(\\yb_n=C|\\xb_i, \\mathbf{W}) = 1- \\sum_{k}^{C-1}P(\\yb_i=k|\\xb_i, \\mathbf{W})\\\\\n",
    "1- \\sum_{k}^{C-1}\\frac{e^{\\xb_i^T\\cdot \\wb_{(k)}}}{1+\\sum_j^{C-1} e^{\\xb_i^T\\cdot \\wb_{(j)}}} = 1-\\frac{\\sum_{k}^{C-1}e^{\\xb_i^T\\cdot \\wb_{(k)}}}{1+\\sum_j^{C-1} e^{\\xb_i^T\\cdot \\wb_{(j)}}}\\\\\n",
    "P(\\yb_i=C|\\xb_i, \\mathbf{W}) = \\frac{1}{1+\\sum_j^{C-1} e^{\\xb_i^\\top\\cdot \\wb_{(j)}}}$$\n",
    "\n",
    "Further reading: https://en.wikipedia.org/wiki/Multinomial_logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Test on the Iris dataset\n",
    "\n",
    "Now let us  test our code on the iris dataset. We load and display the dataset below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multi, labels_multi = helpers.load_ds_iris(sep_l=True, sep_w=True, pet_l=False, pet_w=False, setosa=True, versicolor=True, virginica=True, addbias=True)\n",
    "labels_multi = helpers.label_to_onehot(labels_multi)\n",
    "fig = helpers.scatter2d_multiclass(data_multi, helpers.onehot_to_label(labels_multi), fig=None, fig_size=None, color_map=None,\n",
    "                         legend=True, legend_map=None, grid=False, show=False)\n",
    "\n",
    "num_samples = data_multi.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "data_train = data_multi[rinds[:int(num_samples * fraction_train)]] \n",
    "labels_train = labels_multi[rinds[:int(num_samples * fraction_train)]]  \n",
    "\n",
    "data_test = data_multi[rinds[int(num_samples * fraction_train):]] \n",
    "labels_test= labels_multi[rinds[int(num_samples * fraction_train):]]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_multi = logistic_regression_train_multi(data_train, labels_train, max_iters=10000, lr=1e-3, print_period=1000, plot_period=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_multi = logistic_regression_classify_multi(data_test, weights_multi)\n",
    "print(\"Accuracy is\", accuracy(helpers.onehot_to_label(labels_test), predictions_multi))\n",
    "\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=helpers.onehot_to_label(labels_test), labels_pred=predictions_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Comment on the accuracy. What is the difference between the synthetic dataset and this one?**\n",
    " \n",
    "It is doing a good job at classifying class 0, but not 1 and 2. This is because, for these two features, class 0 is linearly separable from classes 1 and 2, but 1 and 2 are not linearly separable. In fact, the classes 1 and 2 cannot be perfectly separated even with non-linear boundary as some of the samples of both classes completely overlap. In the synthetic dataset, all classes were linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
